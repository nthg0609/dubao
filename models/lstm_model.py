import pandas as pd
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_absolute_percentage_error, r2_score
import matplotlib.pyplot as plt
import seaborn as sns
import os
import random
from datetime import timedelta

# ================== 1. C·ªë ƒë·ªãnh seed reproducibility ==================
def set_seed(seed=42):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.backends.cudnn.deterministic = True
    os.environ['PYTHONHASHSEED'] = str(seed)
set_seed(42)

# ================== 2. L·ªõp Dataset ==================
class TimeSeriesDataset(Dataset):
    def __init__(self, X, y):
        self.X = torch.FloatTensor(X)
        self.y = torch.FloatTensor(y)
    def __len__(self):
        return len(self.X)
    def __getitem__(self, idx):
        return self.X[idx], self.y[idx]

# ================== 3. Model LSTM PyTorch ==================
class LSTMModel(nn.Module):
    def __init__(self, input_size, hidden_size, num_layers, output_size, dropout=0.2):
        super().__init__()
        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout)
        self.fc = nn.Linear(hidden_size, output_size)
    def forward(self, x):
        h0 = torch.zeros(self.lstm.num_layers, x.size(0), self.lstm.hidden_size).to(x.device)
        c0 = torch.zeros(self.lstm.num_layers, x.size(0), self.lstm.hidden_size).to(x.device)
        out, _ = self.lstm(x, (h0, c0))
        # L·∫•y hidden state cu·ªëi c√πng
        return self.fc(out[:, -1, :]) # (batch_size, output_size)

# ================== 4. Ph√¢n t√≠ch th·ªëng k√™ & m√¥ t·∫£ d·ªØ li·ªáu ==================
def analyze_data(df, features):
    print("\n=== PH√ÇN T√çCH TH·ªêNG K√ä D·ªÆ LI·ªÜU ===")
    stat = df[features].describe().T
    stat = stat[['count','mean','std','min','25%','50%','75%','max']]
    stat.columns = [
        "S·ªë l∆∞·ª£ng quan s√°t", "Gi√° tr·ªã trung b√¨nh", "ƒê·ªô l·ªách chu·∫©n", 
        "Gi√° tr·ªã nh·ªè nh·∫•t", "Ph√¢n v·ªã 25%", "Trung v·ªã (50%)",
        "Ph√¢n v·ªã 75%", "Gi√° tr·ªã l·ªõn nh·∫•t"
    ]
    print(stat)
    print("\nKi·ªÉm tra gi√° tr·ªã thi·∫øu:")
    print(df[features].isnull().sum())
    print("\nüîó Ma tr·∫≠n t∆∞∆°ng quan:")
    plt.figure(figsize=(10,7))
    sns.heatmap(df[features].corr(), annot=True, cmap='Blues', fmt=".2f")
    plt.title("Correlation Matrix")
    plt.tight_layout()
    plt.show()
    for col in features:
        plt.figure(figsize=(5,2))
        sns.histplot(df[col], kde=True, bins=50)
        plt.title(f'Ph√¢n ph·ªëi {col}')
        plt.tight_layout()
        plt.show()

# ================== 5. Ti·ªÅn x·ª≠ l√Ω, chu·∫©n h√≥a, t·∫°o window ==================
def preprocess_data(
    df, 
    features, 
    univariate=True, 
    look_back=30, 
    pred_steps=7
):
    # -- Ch·ªâ l·∫•y c√°c c·ªôt c·∫ßn thi·∫øt
    feature_cols = ['Close'] if univariate else features
    scaler_dict = {}
    scaled_data = df[feature_cols].copy()
    # Chu·∫©n h√≥a t·ª´ng bi·∫øn ri√™ng
    for col in feature_cols:
        scaler = MinMaxScaler()
        scaled_data[col] = scaler.fit_transform(df[[col]])
        scaler_dict[col] = scaler
    values = scaled_data.values
    # T·∫°o sliding window cho multi-step
    X, y = [], []
    for i in range(len(values) - look_back - pred_steps + 1):
        X.append(values[i:i+look_back, :])              # shape: (look_back, n_features)
        y.append(values[i+look_back:i+look_back+pred_steps, 0]) # ch·ªâ l·∫•y c·ªôt 'Close' cho nh√£n (d√π multivariate)
    X, y = np.array(X), np.array(y)
    # L·∫•y c√°c ng√†y t∆∞∆°ng ·ª©ng cho y (d√πng ƒë·ªÉ v·∫Ω/truy v·∫øt)
    date_targets = df['Date'].iloc[look_back: look_back+len(y)].reset_index(drop=True)
    return X, y, scaler_dict, date_targets

# ================== 6. Chia train/val/test theo th·ªùi gian ==================
def split_data(X, y, dates, train_ratio=0.8, val_ratio=0.1):
    n = len(X)
    n_train = int(n * train_ratio)
    n_val = int(n * val_ratio)
    X_train, X_val, X_test = X[:n_train], X[n_train:n_train+n_val], X[n_train+n_val:]
    y_train, y_val, y_test = y[:n_train], y[n_train:n_train+n_val], y[n_train+n_val:]
    date_test = dates[n_train+n_val:].reset_index(drop=True)
    return X_train, X_val, X_test, y_train, y_val, y_test, date_test

# ================== 7. Hu·∫•n luy·ªán model ==================
def train_model(model, train_loader, val_loader, criterion, optimizer, epochs, device):
    for epoch in range(1, epochs+1):
        model.train()
        total_loss = 0
        for X_batch, y_batch in train_loader:
            X_batch, y_batch = X_batch.to(device), y_batch.to(device)
            optimizer.zero_grad()
            preds = model(X_batch)
            loss = criterion(preds, y_batch)
            loss.backward()
            optimizer.step()
            total_loss += loss.item()
        model.eval()
        val_loss = 0
        with torch.no_grad():
            for X_batch, y_batch in val_loader:
                X_batch, y_batch = X_batch.to(device), y_batch.to(device)
                val_loss += criterion(model(X_batch), y_batch).item()
        if epoch % 10 == 0 or epoch == 1:
            print(f"Epoch {epoch}/{epochs} ‚Äî Train Loss: {total_loss/len(train_loader):.4f} | Val Loss: {val_loss/len(val_loader):.4f}")

# ================== 8. ƒê√°nh gi√° model v√† ph√¢n t√≠ch l·ªói ==================
def evaluate_and_plot(model, test_loader, scaler_close, device, date_test, pred_steps):
    model.eval()
    all_preds, all_trues = [], []
    with torch.no_grad():
        for X_batch, y_batch in test_loader:
            X_batch = X_batch.to(device)
            preds = model(X_batch).cpu().numpy()
            trues = y_batch.numpy()
            all_preds.append(preds)
            all_trues.append(trues)
    preds = np.vstack(all_preds)     # (N, pred_steps)
    trues = np.vstack(all_trues)     # (N, pred_steps)
    # Inverse transform c·ªôt Close
    preds_inv = scaler_close.inverse_transform(preds)
    trues_inv = scaler_close.inverse_transform(trues)
    # ==== ƒê√°nh gi√° per-step ====
    print("\n=== K·∫æT QU·∫¢ ƒê√ÅNH GI√Å MULTI-STEP ===")
    metrics = []
    for i in range(pred_steps):
        rmse = np.sqrt(mean_squared_error(trues_inv[:,i], preds_inv[:,i]))
        mae = mean_absolute_error(trues_inv[:,i], preds_inv[:,i])
        mape = mean_absolute_percentage_error(trues_inv[:,i], preds_inv[:,i])
        r2 = r2_score(trues_inv[:,i], preds_inv[:,i])
        print(f"B∆∞·ªõc t+{i+1}: RMSE={rmse:.2f} | MAE={mae:.2f} | MAPE={mape:.2%} | R2={r2:.4f}")
        metrics.append([rmse, mae, mape, r2])
    # ==== T·ªïng h·ª£p b·∫£ng k·∫øt qu·∫£ ====
    res_df = pd.DataFrame({
        "Ng√†y": [d+timedelta(days=k) for d in date_test for k in range(1, pred_steps+1)],
        "Gi√° th·ª±c t·∫ø": trues_inv.flatten(),
        "Gi√° d·ª± b√°o": preds_inv.flatten(),
        "Sai s·ªë tuy·ªát ƒë·ªëi": np.abs(trues_inv.flatten() - preds_inv.flatten())
    })
    print("\n== B·∫£ng k·∫øt qu·∫£ (m·∫´u):")
    print(res_df.head(10))
    # ==== Bi·ªÉu ƒë·ªì scatter gi√° th·ª±c vs gi√° d·ª± b√°o ====
    plt.figure(figsize=(6,6))
    plt.scatter(trues_inv.flatten(), preds_inv.flatten(), alpha=0.5)
    plt.xlabel("Gi√° th·ª±c t·∫ø")
    plt.ylabel("Gi√° d·ª± b√°o")
    plt.title("Scatter: Gi√° th·ª±c t·∫ø vs Gi√° d·ª± b√°o (to√†n b·ªô multi-step)")
    plt.grid(True)
    plt.show()
    # ==== Histogram error ====
    plt.figure(figsize=(7,3))
    plt.hist(trues_inv.flatten()-preds_inv.flatten(), bins=40, alpha=0.7)
    plt.title("Histogram sai s·ªë (error = th·ª±c t·∫ø - d·ª± b√°o)")
    plt.xlabel("Error")
    plt.ylabel("S·ªë l∆∞·ª£ng")
    plt.show()
    # ==== Plot t·ª´ng b∆∞·ªõc d·ª± b√°o ====
    for step in range(pred_steps):
        plt.figure(figsize=(11,4))
        plt.plot(trues_inv[:,step], label=f'Th·ª±c t·∫ø t+{step+1}')
        plt.plot(preds_inv[:,step], label=f'D·ª± b√°o t+{step+1}')
        plt.legend()
        plt.title(f"Bi·ªÉu ƒë·ªì d·ª± b√°o b∆∞·ªõc t+{step+1}")
        plt.tight_layout()
        plt.show()
    return res_df, metrics

# ================== 9. H√†m main d√πng cho giao di·ªán ho·∫∑c CLI ==================
def run_lstm_model(
    df=None,
    csv_path=None,
    univariate=True,
    look_back=30,
    pred_steps=7,
    hidden_size=64,
    num_layers=2,
    dropout=0.2,
    batch_size=32,
    epochs=50,
    lr=0.001,
    plot=True
):
    print(f"\nüîµ Kh·ªüi ƒë·ªông m√¥ h√¨nh LSTM {'univariate' if univariate else 'multivariate'} ‚Äî D·ª± b√°o {pred_steps} b∆∞·ªõc, look_back={look_back} ng√†y")
    # ===== 1. Load d·ªØ li·ªáu
    if df is None:
        if csv_path is not None:
            df = pd.read_csv(csv_path)
        else:
            raise ValueError("B·∫°n ph·∫£i truy·ªÅn v√†o df ho·∫∑c csv_path!")
    df['Date'] = pd.to_datetime(df['Date'], dayfirst=True, errors='coerce')
    df = df.sort_values(by='Date')
    df = df[['Date', 'Open', 'High', 'Low', 'Close', 'Volume', 'VWAP']].dropna()
    features = ['Open', 'High', 'Low', 'Close', 'Volume', 'VWAP']

    if plot:
        analyze_data(df, features)
    # ===== 3. T·∫°o window + chu·∫©n h√≥a + l·∫•y scaler
    X, y, scaler_dict, date_targets = preprocess_data(df, features, univariate, look_back, pred_steps)
    scaler_close = scaler_dict['Close']
    # ===== 4. Split train/val/test
    X_train, X_val, X_test, y_train, y_val, y_test, date_test = split_data(X, y, date_targets)
    # ===== 5. DataLoader
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    train_loader = DataLoader(TimeSeriesDataset(X_train, y_train), batch_size=batch_size, shuffle=True)
    val_loader   = DataLoader(TimeSeriesDataset(X_val, y_val), batch_size=batch_size)
    test_loader  = DataLoader(TimeSeriesDataset(X_test, y_test), batch_size=batch_size)
    # ===== 6. Build model
    input_size = X.shape[2]
    model = LSTMModel(input_size, hidden_size, num_layers, pred_steps, dropout=dropout).to(device)
    criterion = nn.MSELoss()
    optimizer = torch.optim.Adam(model.parameters(), lr=lr)
    # ===== 7. Train model
    train_model(model, train_loader, val_loader, criterion, optimizer, epochs, device)
    # ===== 8. ƒê√°nh gi√° v√† ph√¢n t√≠ch l·ªói
    res_df, metrics = evaluate_and_plot(model, test_loader, scaler_close, device, date_test, pred_steps)
    # ===== 9. L∆∞u model (n·∫øu c·∫ßn)
    torch.save(model.state_dict(), f"lstm_{'uni' if univariate else 'multi'}_{look_back}win_{pred_steps}step.pth")
    print("‚úÖ ƒê√£ l∆∞u model v√†o file.")
    return model, res_df, metrics

# ============= 10. V√≠ d·ª• ch·∫°y t·ª´ CLI ho·∫∑c g·ªçi t·ª´ app.py/streamlit =============
if __name__ == "__main__":
    # V√≠ d·ª•: D·ª± b√°o 7 ng√†y, ƒë∆°n bi·∫øn, window 30, multivariate=True/False ƒë·ªÅu ƒë∆∞·ª£c
    run_lstm_model(
        csv_path="AXISBANK.csv",      # ƒê∆∞·ªùng d·∫´n t·ªõi d·ªØ li·ªáu
        univariate=False,             # True: ch·ªâ 'Close', False: ƒëa bi·∫øn (c√°c ƒë·∫∑c tr∆∞ng)
        look_back=30,                 # ƒê·ªô d√†i chu·ªói v√†o
        pred_steps=7,                 # S·ªë ng√†y d·ª± b√°o
        hidden_size=64,               # S·ªë neuron ·∫©n
        num_layers=2,                 # S·ªë l·ªõp LSTM
        dropout=0.2,                  # Dropout gi·ªØa c√°c l·ªõp
        batch_size=32,
        epochs=50,
        lr=0.001,
        plot=True                     # Hi·ªÉn th·ªã bi·ªÉu ƒë·ªì/ph√¢n t√≠ch
    )